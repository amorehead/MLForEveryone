{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Behavioural Cloning from zero in MineRL - Example answers",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_BGlQwngccr"
      },
      "source": [
        "# Example answers for behavioural cloning task\n",
        "\n",
        "## Adapted from "Behavioural Cloning from zero in MineRL - Example answers"\n",
        "\n",
        "This version is filled out version of the behavioural cloning from scratch code.\n",
        "\n",
        "The per-TODO-task (the parts participants fill in) contain small explanations\n",
        "on what should be done there (Comment starting with \"# ANSWER...\").\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mCU5PuhuM6f"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HTScYNljgXv"
      },
      "source": [
        "%%capture\n",
        "# ^ hides output\n",
        "!sudo add-apt-repository -y ppa:openjdk-r/ppa\n",
        "!sudo apt-get purge openjdk-*\n",
        "!sudo apt-get install openjdk-8-jdk\n",
        "!sudo apt-get install xvfb xserver-xephyr vnc4server python-opengl ffmpeg\n",
        "!pip3 install --upgrade minerl\n",
        "!pip3 install pyvirtualdisplay torch numpy\n",
        "!pip3 install -U colabgymrender\n",
        "\n",
        "# Launch virtual display, which is needed for MineRL\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jE1E2ezgOKZV"
      },
      "source": [
        "# Begin by downloading the dataset.\n",
        "import minerl.data.download\n",
        "minerl.data.download(directory='data', environment=\"MineRLTreechop-v0\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8h_teKROD8T2"
      },
      "source": [
        "## Train your model\n",
        "The code below trains your behavioural cloning model. Update the TODO parts to perform the training.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQB93qBIAx_l"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch as th\n",
        "from torch import nn\n",
        "import gym\n",
        "import minerl\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Your task: Implement behavioural cloning for MineRLTreechop-v0.\n",
        "Behavioural cloning is perhaps the simplest way of using a dataset of demonstrations to train an agent:\n",
        "learn to predict what actions they would take, and take those actions.\n",
        "In other machine learning terms, this is almost like building a classifier to classify observations to\n",
        "different actions, and taking those actions.\n",
        "For simplicity, we build a limited set of actions (\"agent actions\"), map dataset actions to these actions\n",
        "and train on the agent actions. During evaluation, we transform these agent actions (integerse) back into\n",
        "MineRL actions (dictionaries).\n",
        "To do this task, fill in the \"TODO\"s and remove `raise NotImplementedError`s.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    \"\"\"\n",
        "    :param input_shape: A three-item tuple telling image dimensions in (C, H, W)\n",
        "    :param output_dim: Dimensionality of the output vector\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_shape, output_dim):\n",
        "        super().__init__()\n",
        "        # TODO Create a torch neural network here to turn images (of shape `input_shape`) into\n",
        "        #      a vector of shape `output_dim`. This output_dim matches number of available actions.\n",
        "        #      See examples of doing CNN networks here https://pytorch.org/tutorials/beginner/nn_tutorial.html#switch-to-cnn\n",
        "        # ANSWER See below. Any regular CNN will work.\n",
        "        n_input_channels = input_shape[0]\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(n_input_channels, 32, kernel_size=8, stride=4, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "\n",
        "        # Compute shape by doing one forward pass\n",
        "        # ANSWER you can use this trick to get CNN output shape dynamically \n",
        "        with th.no_grad():\n",
        "            n_flatten = self.cnn(th.zeros(1, *input_shape)).shape[1]\n",
        "\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(n_flatten, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
        "        # TODO with the layers you created in __init__, transform the `observations` (a tensor of shape (B, C, H, W)) to\n",
        "        #      a tensor of shape (B, D), where D is the `output_dim`\n",
        "        # ANSWER good olde' forward pass\n",
        "        return self.linear(self.cnn(observations))\n",
        "\n",
        "\n",
        "def environment_action_batch_to_agent_actions(dataset_actions):\n",
        "    \"\"\"\n",
        "    Turn a batch of actions from environment (from BufferedBatchIterator) to a numpy\n",
        "    array of agent actions.\n",
        "    Agent actions _have to_ start from 0 and go up from there!\n",
        "    For MineRLTreechop, you want to have actions for the following at the very least:\n",
        "    - Forward movement\n",
        "    - Jumping\n",
        "    - Turning camera left, right, up and down\n",
        "    - Attack\n",
        "    For example, you could have seven agent actions that mean following:\n",
        "    0 = forward\n",
        "    1 = jump + forward\n",
        "    2 = turn camera left\n",
        "    3 = turn camera right\n",
        "    4 = turn camera up\n",
        "    5 = turn camera down\n",
        "    6 = attack\n",
        "    This should match `agent_action_to_environment`, by converting dictionary\n",
        "    actions into individual integeres.\n",
        "    If dataset action (dict) does not have a mapping to agent action (int),\n",
        "    then set it \"-1\"\n",
        "    \"\"\"\n",
        "    # There are dummy dimensions of shape one\n",
        "    batch_size = len(dataset_actions[\"camera\"])\n",
        "    actions = np.zeros((batch_size,), dtype=np.int)\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        # ANSWER map each action dict to actions described above,\n",
        "        \n",
        "        # ANSWER first check camera movement with a small margin\n",
        "        # (Make camera movement be most important)\n",
        "        if dataset_actions[\"camera\"][i][0] < -5:\n",
        "            actions[i] = 2\n",
        "        elif dataset_actions[\"camera\"][i][0] > 5:\n",
        "            actions[i] = 3\n",
        "        elif dataset_actions[\"camera\"][i][1] > 5:\n",
        "            actions[i] = 4\n",
        "        elif dataset_actions[\"camera\"][i][1] < -5:\n",
        "            actions[i] = 5\n",
        "        # ANSWER two different actions: \"forward\" and \"forward-jump\"\n",
        "        elif dataset_actions[\"forward\"][i] == 1:\n",
        "            if dataset_actions[\"jump\"][i] == 1:\n",
        "                actions[i] = 1\n",
        "            else:\n",
        "                actions[i] = 0\n",
        "        # ANSWER attack action\n",
        "        elif dataset_actions[\"attack\"][i] == 1:\n",
        "            actions[i] = 6\n",
        "        else:\n",
        "            # No reasonable mapping (would be no-op)\n",
        "            actions[i] = -1\n",
        "    return actions\n",
        "\n",
        "\n",
        "def train():\n",
        "    # Path to where MineRL dataset resides (should contain \"MineRLTreechop-v0\" directory)\n",
        "    DATA_DIR = \".\"\n",
        "    # How many times we train over dataset and how large batches we use.\n",
        "    # Larger batch size takes more memory but generally provides stabler learning.\n",
        "    EPOCHS = 1\n",
        "    BATCH_SIZE = 32\n",
        "\n",
        "    # TODO create data iterators for going over MineRL Treechop-v0 data using BufferedBatchIterator\n",
        "    #      https://minerl.readthedocs.io/en/latest/tutorials/data_sampling.html#sampling-the-dataset-with-buffered-batch-iter\n",
        "    # ANSWER see below. Minimal dataset is downloaded automatically\n",
        "    data = minerl.data.make(\"MineRLTreechop-v0\", data_dir=\"data\")\n",
        "    iterator = minerl.data.BufferedBatchIter(data, buffer_target_size=10000)\n",
        "\n",
        "    number_of_actions = 7\n",
        "    # TODO we need to tell the network how many possible actions there are,\n",
        "    #      so assign the value in above variable\n",
        "    # ANSWER the number is defined by the action-mapping function above\n",
        "    # (number of possible integers)\n",
        "\n",
        "    network = ConvNet((3, 64, 64), number_of_actions).cuda()\n",
        "    # TODO create optimizer and loss functions for training\n",
        "    #      see examples here https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html\n",
        "    # ANSWER below.\n",
        "    optimizer = th.optim.Adam(network.parameters())\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "    iter_count = 0\n",
        "    losses = []\n",
        "    for dataset_obs, dataset_actions, _, _, _ in tqdm(iterator.buffered_batch_iter(num_epochs=EPOCHS, batch_size=BATCH_SIZE)):\n",
        "        # We only use camera observations here\n",
        "        obs = dataset_obs[\"pov\"].astype(np.float32)\n",
        "        # Transpose observations to be channel-first (BCHW instead of BHWC)\n",
        "        obs = obs.transpose(0, 3, 1, 2)\n",
        "        # Normalize observations, otherwise the neural network will get spooked\n",
        "        obs /= 255.0\n",
        "\n",
        "        # Turn dataset actions into agent actions\n",
        "        actions = environment_action_batch_to_agent_actions(dataset_actions)\n",
        "        assert actions.shape == (obs.shape[0],), \"Array from environment_action_batch_to_agent_actions should be of shape {}\".format((obs.shape[0],))\n",
        "\n",
        "        # Remove samples that had no corresponding action\n",
        "        mask = actions != -1\n",
        "        obs = obs[mask]\n",
        "        actions = actions[mask]\n",
        "\n",
        "        # TODO perform optimization step:\n",
        "        # - Predict actions using the neural network (input is `obs`)\n",
        "        # - Compute loss with the predictions and true actions. Store loss into variable `loss`\n",
        "        # - Use optimizer to do a single update step\n",
        "        # See https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html \n",
        "        # for a tutorial\n",
        "        # NOTE: Variables `obs` and `actions` are numpy arrays. You need to convert them into torch tensors.\n",
        "        # ANSWER, below\n",
        "        # Obtain logits of each action\n",
        "        logits = network(th.from_numpy(obs).float().cuda())\n",
        "\n",
        "        # Minimize cross-entropy with target labels.\n",
        "        # We could also compute the probability of demonstration actions and\n",
        "        # maximize them.\n",
        "        loss = loss_function(logits, th.from_numpy(actions).long().cuda())\n",
        "\n",
        "        # Standard PyTorch update\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Keep track of how training is going by printing out the loss\n",
        "        iter_count += 1\n",
        "        losses.append(loss.item())\n",
        "        if (iter_count % 1000) == 0:\n",
        "            mean_loss = sum(losses) / len(losses)\n",
        "            tqdm.write(\"Iteration {}. Loss {:<10.3f}\".format(iter_count, mean_loss))\n",
        "            losses.clear()\n",
        "\n",
        "    # Store the network\n",
        "    th.save(network, \"behavioural_cloning.pth\")\n",
        "\n",
        "train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tFxyp5wwPp7"
      },
      "source": [
        "## Enjoy trained model\n",
        "\n",
        "After training the model, fix the code below to load up the trained model and play games with it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5Q5VgwCwM2d"
      },
      "source": [
        "def agent_action_to_environment(noop_action, agent_action):\n",
        "    \"\"\"\n",
        "    Turn an agent action (an integer) into an environment action.\n",
        "    This should match `environment_action_batch_to_agent_actions`,\n",
        "    e.g. if attack=1 action was mapped to agent_action=0, then agent_action=0\n",
        "    should be mapped back to attack=1.\n",
        "\n",
        "    noop_action is a MineRL action that does nothing. You may want to\n",
        "    use this as a template for the action you return.\n",
        "    \"\"\"\n",
        "    # ANSWER this should be reverse of `environment_action_batch_to_agent_actions`\n",
        "    action = noop_action\n",
        "    if agent_action == 0:\n",
        "        action[\"forward\"] = 1\n",
        "    elif agent_action == 1:\n",
        "        action[\"forward\"] = 1\n",
        "        action[\"jump\"] = 1\n",
        "    elif agent_action == 2:\n",
        "        action[\"camera\"][0] = -5\n",
        "    elif agent_action == 3:\n",
        "        action[\"camera\"][0] = 5\n",
        "    elif agent_action == 4:\n",
        "        action[\"camera\"][1] = -5\n",
        "    elif agent_action == 5:\n",
        "        action[\"camera\"][1] = 5\n",
        "    elif agent_action == 6:\n",
        "        action[\"attack\"] = 1\n",
        "    return action\n",
        "\n",
        "\n",
        "# Load up the trained network\n",
        "network = th.load(\"behavioural_cloning.pth\").cuda()\n",
        "\n",
        "env = gym.make('MineRLTreechop-v0')\n",
        "\n",
        "# Play 3 games with the model\n",
        "for game_i in range(3):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    reward_sum = 0\n",
        "    while not done:\n",
        "        # TODO Process the observation:\n",
        "        #   - Take only the camera observation\n",
        "        #   - Add/remove batch dimensions\n",
        "        #   - Transpose image (needs to be channels-last)\n",
        "        #   - Normalize image\n",
        "        #   - Store network output to `logits`\n",
        "        # For hints, see what preprocessing was done during training\n",
        "        # ANSWER see below\n",
        "        # Transpose, add batch dim, normalize, turn into torch tensor\n",
        "        obs = th.from_numpy(obs[\"pov\"].transpose(2, 0, 1)[None].astype(np.float32) / 255.0).cuda()\n",
        "        logits = network(obs)[0]\n",
        "        # Turn logits into probabilities\n",
        "        probabilities = th.softmax(logits, dim=0)\n",
        "        # Into numpy\n",
        "        probabilities = probabilities.detach().cpu().numpy()\n",
        "        # TODO Pick an action based from the probabilities above.\n",
        "        # The `probabilities` vector tells the probability of choosing one of the agent actions.\n",
        "        # You have two options:\n",
        "        # 1) Pick action with the highest probability\n",
        "        # 2) Sample action based on probabilities\n",
        "        # Option 2 works better emperically.\n",
        "        # ANSWER pick action randomly below\n",
        "        agent_action = np.random.choice(np.arange(probabilities.shape[0]), p=probabilities)\n",
        "\n",
        "        noop_action = env.action_space.noop()\n",
        "        environment_action = agent_action_to_environment(noop_action, agent_action)\n",
        "\n",
        "        obs, reward, done, info = env.step(environment_action)\n",
        "        reward_sum += reward\n",
        "    print(\"Game {}, total reward {}\".format(game_i, reward_sum))\n",
        "\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
